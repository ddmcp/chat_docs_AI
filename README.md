# üöÄ Airflow & MinIO: Intelligent PDF RAG Pipeline

***Transform your static PDF collection into a searchable, interactive knowledge base using Airflow and Vector Embeddings.***

## üöÄ Getting Started

Follow these simple steps to get your intelligent PDF pipeline up and running.

### 1. Launch Infrastructure
Spin up all services (Postgres, MinIO, Airflow, Qdrant, and Microservices):
```bash
docker-compose up -d
```

### 2. Configure Airflow
Create your admin credentials to access the dashboard:
```bash
docker exec -it airflow-api-server airflow users create \
  --username admin --password admin \
  --firstname Admin --lastname Admin \
  --role Admin \
  --email admin@example.com
```

### 3. Upload Your Documents
1.  Open **[MinIO Console](http://localhost:9001)** (Login: `minioadmin` / `minioadmin`).
2.  Create a bucket named **`test1`**.
3.  Upload your PDF files into this bucket.

### 4. Process the PDFs
1.  Open the **[Airflow UI](http://localhost:8080/dags)** (Login: `admin` / `admin`).
2.  Locate the **`minio_pdf_processor_dag`**.
3.  **Unpause** it and click **Trigger** to start extracting data and generating embeddings.

### 5. Chat with Your Data
Once processing is complete, test your RAG pipeline via the **[Search API Swagger](http://localhost:8003/docs#/default/find_by_text_search_post)**.

**Example Query:**
```json
{
  "query": "UK-based businesses",
  "limit": 5
}
```

---

## üìñ Project Overview

This project implements a complete **Retrieval-Augmented Generation (RAG)** pipeline. It automates the ingestion of PDF documents, extracts semantic information, and enables natural language querying.

- **Orchestration**: Managed by **Apache Airflow 3.x**.
- **Storage**: Files in **MinIO**, Metadata in **Postgres**, Vectors in **Qdrant**.
- **AI**: Embeddings via **Sentence Transformers**.

### üèó Architecture At a Glance

The system is composed of the Airflow ecosystem and two specialized microservices:
1.  **Extraction Microservice**: Downloads PDFs from MinIO, extracts text/metadata, and saves to SQL+Vector DBs.
2.  **Search Microservice**: Provides a high-level API for semantic search across the processed knowledge base.

---

## Airflow Orchestration

Airflow is the heart of the project, coordinating data movement and processing.

### üìã DAG Catalog

- **`minio_pdf_processor_dag`**: The primary pipeline. It monitors MinIO buckets for new PDF uploads and triggers the extraction microservice to process them in real-time.
- **`hello_world_dag`**: A simple diagnostic DAG to verify scheduler health.
- **`debug_test_dag`**: Used for testing internal API connections and core Airflow variables.

### üõ† Working with DAGs

#### Adding New Logic
1.  Place your `.py` files in the `./dags` folder.
2.  The **DAG Processor** will automatically detect and serialize them within seconds.
3.  Check the status via CLI:
    ```bash
    docker exec -it airflow-api-server airflow dags list
    ```

#### Monitoring & Logs
Tracking task execution is critical. Use these commands to inspect the scheduler's behavior:
```bash
# Check if the scheduler sees your file
docker logs airflow-scheduler | grep your_dag_name.py

# Get logs for a specific task instance
docker exec -it airflow-scheduler airflow tasks logs <dag_id> <task_id> <run_id>
```

#### Manual Triggering & Testing
Sometimes you need to bypass the sensor and run a DAG immediately:
```bash
# Test a specific task without running the whole DAG
docker exec -it airflow-api-server airflow tasks test <dag_id> <task_id> 2024-01-01

# Trigger a full DAG run
docker exec -it airflow-scheduler airflow dags trigger <dag_id>
```

---

## üêû Developer Experience (DX)

### Debugging DAGs in VS Code
The environment is pre-configured for remote debugging using `debugpy`.

1.  Add this to your `.vscode/launch.json`:
    ```json
    {
        "version": "0.2.0",
        "configurations": [
            {
                "name": "Airflow: Attach to Docker",
                "type": "debugpy",
                "request": "attach",
                "connect": { "host": "localhost", "port": 5678 },
                "pathMappings": [
                    { "localRoot": "${workspaceFolder}/dags", "remoteRoot": "/opt/airflow/dags" }
                ]
            }
        ]
    }
    ```
2.  Run the task with the debug flag:
    ```bash
    docker exec -it -e AIRFLOW_DEBUG=true airflow-scheduler airflow tasks test <dag_id> <task_id> 2026-01-01
    ```

---

## üè∑ Tags
`Airflow 3.x` ‚Ä¢ `MinIO` ‚Ä¢ `Qdrant` ‚Ä¢ `PostgreSQL` ‚Ä¢ `RAG` ‚Ä¢ `Docker` ‚Ä¢ `Python` ‚Ä¢ `FastAPI`